name: "SQLite AI - AI search for documents"
description: "Parses document files to create a database on your SQLite Cloud project."
author: "SQLite AI Team"

inputs:
  connection_string:
    description: The SQLite Cloud project connection string.
    required: true
  base_url:
    description: Your website's documentation base url.
    required: true
  database_name:
    description: The name of the database to use on SQLite Cloud
    required: true
  database_path:
    description: The local path of the database file to upload to SQLite Cloud.
    required: true
  source_files:
    description: The path of the files, by default it will parse every file recursively starting from the working directory.
    required: false
    default: $(pwd)
  hf_model_id: 
    description: The Hugging Face model ID to use for generating embeddings.
    required: false
    default: "unsloth/embeddinggemma-300m-GGUF"
  hf_gguf_file:
    description: The GGUF file name for the Hugging Face model.
    required: false
    default: "embeddinggemma-300M-Q8_0.gguf"
  hf_gguf_update_date:
    description: The date to force update the GGUF model cache, format YYYYMMDD.
    required: false
    default: "20250904"
  hf_model_local_path:
    description: The local path to store the downloaded Hugging Face model.
    required: false
    default: "./models"
  

branding:
  icon: "search"
  color: "blue"

runs:
  using: "composite"
  steps:

    - name: Set GitHub Path
      run: echo "${{ github.action_path }}/src" >> $GITHUB_PATH
      shell: bash

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.12"

    - name: Install SQLite RAG
      run: |
        python -m pip install --upgrade pip
        pip install \
          -i https://test.pypi.org/simple/ \
          --extra-index-url https://pypi.org/simple \
          sqlite-rag
      shell: bash

      # Cache the downloaded model between workflows
    - name: Restore GGUF model cache
      uses: actions/cache@v4
      id: cache-model
      with:
        path: ${{ inputs.hf_model_local_path }}
        # Change the HF_GGUF_UPDATE_DATE variable to force update the cache
        key: gguf-${{ inputs.hf_model_id }}-${{ inputs.hf_gguf_file }}-${{ inputs.hf_gguf_update_date }}
        restore-keys: |
          gguf-${{ inputs.hf_model_id }}-${{ inputs.hf_gguf_file }}-${{ inputs.hf_gguf_update_date }}-

    - name: Download Hugging Face model
      run: |
        sqlite-rag download-model "${{ inputs.hf_model_id }}" "${{ inputs.hf_gguf_file }}" --local-dir="${{ inputs.hf_model_local_path }}"
      if: steps.cache-model.outputs.cache-hit != 'true'
      shell: bash

    - name: Parse documents and create the database
      run: |
        sqlite-rag add \
          --recursive "${{ inputs.source_files }}" \
          --metadata '{"base_url": "${{ inputs.base_url }}"}'
      shell: bash

    - name: Upload the database to SQLite Cloud
      run: bash $GITHUB_ACTION_PATH/upload.sh
      env:
        GITHUB_ACTION_PATH: ${{ github.action_path }}
        CONNECTION_STRING: ${{ inputs.connection_string }}
        DATABASE_PATH: ${{ inputs.database_path }}
        DATABASE: ${{ inputs.database_name }}
      shell: bash
